{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T14:04:57.213785Z",
     "start_time": "2025-03-12T14:04:52.017843Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\transformers\\utils\\hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:00:31.795767Z",
     "start_time": "2025-03-12T14:00:31.766741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = Dataset.load_from_disk(\"../../02_实战演练篇/09_对话机器人/alpaca_data_zh\") \n",
    "dataset\n",
    "dataset[0:3]"
   ],
   "id": "fc7f295d1a28c859",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'],\n",
       " 'input': ['', '输入：4/16', ''],\n",
       " 'instruction': ['保持健康的三个提示。', '解释为什么以下分数等同于1/4', '朱利叶斯·凯撒是如何死亡的？']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:00:54.118510Z",
     "start_time": "2025-03-12T14:00:53.868359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/Study_Date/Modelscope/cache/modelscope/Llama-2-7b-ms\")\n",
    "tokenizer"
   ],
   "id": "24ec8efad5dc24fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='D:/Study_Date/Modelscope/cache/modelscope/Llama-2-7b-ms', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:01:05.336727Z",
     "start_time": "2025-03-12T14:01:05.314727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = 2\n",
    "tokenizer"
   ],
   "id": "b556178ffce12532",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='D:/Study_Date/Modelscope/cache/modelscope/Llama-2-7b-ms', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:01:35.700818Z",
     "start_time": "2025-03-12T14:01:35.691598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 1024\n",
    "    inputs_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + examples[\"instruction\"], examples[\"input\"]]).strip() + \"\\n\\nAssistant: \",add_special_tokens=False)\n",
    "    response = tokenizer(examples[\"output\"], add_special_tokens=False)\n",
    "    inputs_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id] # 模型只需要学习response\n",
    "    if len(inputs_ids) > max_length:\n",
    "        inputs_ids = inputs_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    return {\"input_ids\": inputs_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ],
   "id": "4cfa036bf4da871c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:01:45.473943Z",
     "start_time": "2025-03-12T14:01:45.320354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function,remove_columns=dataset.column_names)\n",
    "tokenized_datasets"
   ],
   "id": "f6c3fdeec80a579a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:01:49.827904Z",
     "start_time": "2025-03-12T14:01:49.806895Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(tokenized_datasets[0][\"input_ids\"])",
   "id": "8291928893928236",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 保持健康的三个提示。\\n\\nAssistant:  以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:05:09.609754Z",
     "start_time": "2025-03-12T14:05:08.746134Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForCausalLM.from_pretrained(\"D:/Study_Date/Modelscope/cache/modelscope/Llama-2-7b-ms\",low_cpu_mem_usage=True,torch_dtype=torch.float16)",
   "id": "87050e8664173661",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17ceb8c7f96f470d94f105d6bbdc203e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "页面文件太小，无法完成操作。 (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mD:/Study_Date/Modelscope/cache/modelscope/Llama-2-7b-ms\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    565\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    566\u001B[0m     )\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    570\u001B[0m )\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\transformers\\modeling_utils.py:3838\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3829\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[0;32m   3831\u001B[0m     (\n\u001B[0;32m   3832\u001B[0m         model,\n\u001B[0;32m   3833\u001B[0m         missing_keys,\n\u001B[0;32m   3834\u001B[0m         unexpected_keys,\n\u001B[0;32m   3835\u001B[0m         mismatched_keys,\n\u001B[0;32m   3836\u001B[0m         offload_index,\n\u001B[0;32m   3837\u001B[0m         error_msgs,\n\u001B[1;32m-> 3838\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3839\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3840\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3841\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloaded_state_dict_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# XXX: rename?\u001B[39;49;00m\n\u001B[0;32m   3842\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3843\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3845\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3846\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_fast_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_fast_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3848\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3849\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3850\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3851\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3852\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3853\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgguf_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgguf_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3855\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3857\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[0;32m   3858\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\transformers\\modeling_utils.py:4278\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001B[0m\n\u001B[0;32m   4276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shard_file \u001B[38;5;129;01min\u001B[39;00m disk_only_shard_files:\n\u001B[0;32m   4277\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m-> 4278\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_quantized\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_quantized\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4280\u001B[0m \u001B[38;5;66;03m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001B[39;00m\n\u001B[0;32m   4281\u001B[0m \u001B[38;5;66;03m# matching the weights in the model.\u001B[39;00m\n\u001B[0;32m   4282\u001B[0m mismatched_keys \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m _find_mismatched_keys(\n\u001B[0;32m   4283\u001B[0m     state_dict,\n\u001B[0;32m   4284\u001B[0m     model_state_dict,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4288\u001B[0m     ignore_mismatched_sizes,\n\u001B[0;32m   4289\u001B[0m )\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\transformers\\modeling_utils.py:516\u001B[0m, in \u001B[0;36mload_state_dict\u001B[1;34m(checkpoint_file, is_quantized)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    512\u001B[0m \u001B[38;5;124;03mReads a PyTorch checkpoint file, returning properly formatted errors if they arise.\u001B[39;00m\n\u001B[0;32m    513\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    514\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m checkpoint_file\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.safetensors\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m is_safetensors_available():\n\u001B[0;32m    515\u001B[0m     \u001B[38;5;66;03m# Check format of the archive\u001B[39;00m\n\u001B[1;32m--> 516\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43msafe_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframework\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m    517\u001B[0m         metadata \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mmetadata()\n\u001B[0;32m    518\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m metadata\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlx\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "\u001B[1;31mOSError\u001B[0m: 页面文件太小，无法完成操作。 (os error 1455)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:05:05.361226Z",
     "start_time": "2025-03-12T14:05:04.710112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ],
   "id": "b4c1506f7cdee70",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, param \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mnamed_parameters():\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(name, param\u001B[38;5;241m.\u001B[39mdtype)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:14:28.298706Z",
     "start_time": "2025-03-12T13:14:28.284708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM)\n",
    "config"
   ],
   "id": "2ceba12bc9ac3978",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:14:28.408108Z",
     "start_time": "2025-03-12T13:14:28.299707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_peft_model(model, config)\n",
    "config\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ],
   "id": "dfda4ccb6a44c2e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.norm.weight torch.float16\n",
      "base_model.model.lm_head.weight torch.float16\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:14:28.423116Z",
     "start_time": "2025-03-12T13:14:28.409112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.enable_input_require_grads()# 开启梯度检查点时候必须要开启这个选项\n",
    "model = model.half()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ],
   "id": "83669e708fc981c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.0.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.1.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.2.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.3.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.4.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.5.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.6.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.7.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.8.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.9.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.10.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.11.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.12.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.13.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.14.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.15.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.16.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.17.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.18.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.19.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.20.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.21.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.22.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.23.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.24.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.25.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.26.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.27.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.28.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.29.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.30.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight torch.float16\n",
      "base_model.model.model.layers.31.input_layernorm.weight torch.float16\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.model.norm.weight torch.float16\n",
      "base_model.model.lm_head.weight torch.float16\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:14:45.205741Z",
     "start_time": "2025-03-12T13:14:28.424109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./01_半精度训练\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    save_steps=20,\n",
    "    gradient_checkpointing=True,\n",
    "    adam_epsilon=1e-4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_datasets.select(range(30)),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ],
   "id": "9ce8c8410ec0803b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T13:18:11.483062Z",
     "start_time": "2025-03-12T13:14:45.213741Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "24b43e41867d7b60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "D:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\peft\\utils\\save_and_load.py:195: UserWarning: Could not find a config file in D:/Study_Date/Modelscope/cache/modelscope/Llama-2-7b-ms - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.39236052831014, metrics={'train_runtime': 204.5355, 'train_samples_per_second': 0.147, 'train_steps_per_second': 0.015, 'total_flos': 176607400919040.0, 'train_loss': 1.39236052831014, 'epoch': 0.8})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-12T13:20:21.639912Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "604d8044684d4c40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ae34a4fd49e889ac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
