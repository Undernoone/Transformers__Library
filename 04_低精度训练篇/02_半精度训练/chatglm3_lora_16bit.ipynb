{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGLM3 Lora 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:35:47.989721Z",
     "start_time": "2025-03-13T10:35:43.505172Z"
    }
   },
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\transformers\\utils\\hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:36:01.643938Z",
     "start_time": "2025-03-13T10:36:01.622578Z"
    }
   },
   "source": [
    "ds = Dataset.load_from_disk(\"../data/alpaca_data_zh/\")\n",
    "ds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:36:02.733155Z",
     "start_time": "2025-03-13T10:36:02.713953Z"
    }
   },
   "source": [
    "ds[:3]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'],\n",
       " 'input': ['', '输入：4/16', ''],\n",
       " 'instruction': ['保持健康的三个提示。', '解释为什么以下分数等同于1/4', '朱利叶斯·凯撒是如何死亡的？']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:36:21.288900Z",
     "start_time": "2025-03-13T10:36:21.143166Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"d:/Pretrained_models/ZhipuAI/chatglm3-6b-base/\", trust_remote_code=True)\n",
    "tokenizer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='D:/Study_Date/Modelscope/cache/modelscope/Llama-2-7b-ms', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:36:25.227437Z",
     "start_time": "2025-03-13T10:36:25.214357Z"
    }
   },
   "source": [
    "tokenizer(tokenizer.eos_token), tokenizer.eos_token_id"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [1, 2], 'attention_mask': [1, 1]}, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:36:28.655722Z",
     "start_time": "2025-03-13T10:36:28.639718Z"
    }
   },
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = \"\\n\".join([example[\"instruction\"], example[\"input\"]]).strip()     # query\n",
    "    instruction = tokenizer.build_chat_input(instruction, history=[], role=\"user\")  # [gMASK]sop<|user|> \\n query<|assistant|>\n",
    "    response = tokenizer(\"\\n\" + example[\"output\"], add_special_tokens=False)        # \\n response, 缺少eos token\n",
    "    input_ids = instruction[\"input_ids\"][0].numpy().tolist() + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"][0].numpy().tolist() + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"][0].numpy().tolist()) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:36:31.622861Z",
     "start_time": "2025-03-13T10:36:31.223990Z"
    }
   },
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/26858 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bc685f5891041d5846970973c528d63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizerFast' object has no attribute 'build_chat_input'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m tokenized_ds \u001B[38;5;241m=\u001B[39m \u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m tokenized_ds\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\datasets\\arrow_dataset.py:602\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    600\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    601\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 602\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    603\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    604\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    605\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\datasets\\arrow_dataset.py:567\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    560\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    564\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    565\u001B[0m }\n\u001B[0;32m    566\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 567\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    568\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    569\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\datasets\\arrow_dataset.py:3161\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3156\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[0;32m   3157\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3158\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3159\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3160\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3161\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3162\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3163\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\datasets\\arrow_dataset.py:3522\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3520\u001B[0m _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   3521\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[1;32m-> 3522\u001B[0m     example \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3523\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m update_data:\n\u001B[0;32m   3524\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\Software\\Anaconda\\envs\\LLM\\lib\\site-packages\\datasets\\arrow_dataset.py:3421\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3419\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   3420\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 3421\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[0;32m   3422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[0;32m   3423\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3424\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[0;32m   3425\u001B[0m     }\n",
      "Cell \u001B[1;32mIn[8], line 5\u001B[0m, in \u001B[0;36mprocess_func\u001B[1;34m(example)\u001B[0m\n\u001B[0;32m      3\u001B[0m input_ids, attention_mask, labels \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[0;32m      4\u001B[0m instruction \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([example[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstruction\u001B[39m\u001B[38;5;124m\"\u001B[39m], example[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m]])\u001B[38;5;241m.\u001B[39mstrip()     \u001B[38;5;66;03m# query\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m instruction \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_chat_input\u001B[49m(instruction, history\u001B[38;5;241m=\u001B[39m[], role\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# [gMASK]sop<|user|> \\n query<|assistant|>\u001B[39;00m\n\u001B[0;32m      6\u001B[0m response \u001B[38;5;241m=\u001B[39m tokenizer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m example[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m], add_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)        \u001B[38;5;66;03m# \\n response, 缺少eos token\u001B[39;00m\n\u001B[0;32m      7\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m instruction[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mtolist() \u001B[38;5;241m+\u001B[39m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39meos_token_id]\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'LlamaTokenizerFast' object has no attribute 'build_chat_input'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_ds[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "新版本中需要将modeling_chatglm源码中的613行部分进行调整，代码如下：\n",
    "\n",
    "```\n",
    "if not kv_caches:\n",
    "    kv_caches = [None for _ in range(self.num_layers)]\n",
    "else:\n",
    "    kv_caches = kv_caches[1]\n",
    "```\n",
    "\n",
    "如果不进行调整，后续chat阶段会报错\n",
    "\"\"\"\n",
    "# 多卡情况，可以去掉device_map=\"auto\"，否则会将模型拆开\n",
    "model = AutoModelForCausalLM.from_pretrained(\"d:/Pretrained_models/ZhipuAI/chatglm3-6b-base/\", trust_remote_code=True, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "config = LoraConfig(target_modules=[\"query_key_value\"], modules_to_save=[\"post_attention_layernorm\"])\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    remove_unused_columns=False,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds.select(range(6000)),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open(\"./chatbot/checkpoint-187/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "    for key in f.keys():\n",
    "        if \".0.post_attention_layernorm\" in key:\n",
    "            print(key)\n",
    "            print(f.get_tensor(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(model.chat(tokenizer, \"数学考试怎么考高分？\", history=[])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
