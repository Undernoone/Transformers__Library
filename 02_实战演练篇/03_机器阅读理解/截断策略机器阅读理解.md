# 机器阅读理解

这是一个 **机器阅读理解** (Machine Reading Comprehension, MRC) 任务的训练脚本，基于 **Hugging Face Transformers** 框架。它使用 **CMRC 2018** 数据集，并训练一个 **MacBERT** 预训练模型进行问答任务。下面是对代码的 **逐行解析**。

## **加载数据和 Tokenizer**

```python
datasets = load_dataset("hfl/cmrc2018")  # 一个机器阅读理解数据集，包括id，内容，问题，答案
tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-macbert-base")
```

## 处理数据

```python
def process_function(examples):
    tokenized_examples = tokenizer(text=examples["question"],
                                   text_pair=examples["context"],
                                   max_length=512,
                                   truncation="only_second",
                                   padding="max_length",
                                   return_offsets_mapping=True)
```

- `text=examples["question"]`: 设定输入 **问题** (`question`) 作为主文本。

- `text_pair=examples["context"]`: 设定输入 **文章** (`context`) 作为辅助文本。

- `max_length=512`: 设定最大长度为 **512**（BERT 的默认最大长度）。

- ```
  truncation="only_second"
  ```

  - 由于 `text_pair` 设定了 `context`，当超出 `max_length` 时，只截断 **context**（即 `question` 保持完整）。

- ```
  padding="max_length"
  ```

  - 统一长度，填充至 **512** 个 token。

- ```
  return_offsets_mapping=True
  ```

  - 返回 **offset_mapping**（字符索引映射），用于定位答案在 **context** 中的起始位置。

## **计算答案的起止位置**

```python
offset_mapping = tokenized_examples.pop("offset_mapping")
    start_positions = []
    end_positions = []
    for index, offsets in enumerate(offset_mapping):
        answer = examples["answers"][index]
        start_char = answer["answer_start"][0]
        end_char = start_char + len(answer["text"][0])
```

- `offset_mapping.pop("offset_mapping")`
  - `offset_mapping` 记录每个 token 对应的字符位置索引。
- 遍历 `offset_mapping` 并获取答案信息
  - `answer["answer_start"][0]`: 获取答案在 **context** 中的起始字符索引。
  - `end_char`: 计算答案的 **终止字符索引**。

## **确定答案起止 token 位置**

```python
		context_start = tokenized_examples.sequence_ids(index).index(1)
        context_end = tokenized_examples.sequence_ids(index).index(None, context_start)-1
```

- ```
  sequence_ids(index)
  ```

  获取 token 对应的文本类型

  - `0` 代表 `question`
  - `1` 代表 `context`
  - `None` 代表特殊 token（[CLS], [SEP]）

- `context_start`: `context` 的起始 token 索引。

- `context_end`: `context` 的终止 token 索引。

## **判断答案是否在 `context` 范围内**

```python
if offsets[context_end][1] < start_char or offsets[context_start][0] > end_char:
            start_token_pos = 0
            end_token_pos = 0
```

- 如果 `context` 的 token 范围 **不包含答案**，则将起始和终止位置设为 `0`（无效答案）。

## **计算答案的 token 位置**

```python
        else:
            token_id = context_start
            while token_id <= context_end and offsets[token_id][0] < start_char:
                token_id += 1
            start_token_pos = token_id

            token_id = context_end
            while token_id >= context_start and offsets[token_id][1] > end_char:
                token_id -= 1
            end_token_pos = token_id
```

- 遍历 `offsets`，找到 **第一个包含答案起始字符的 token**，标记为 `start_token_pos`。
- 反向遍历 `offsets`，找到 **最后一个包含答案终止字符的 token**，标记为 `end_token_pos`。

## **存储答案 token 位置**

```python
        start_positions.append(start_token_pos)
        end_positions.append(end_token_pos)
    tokenized_examples["start_positions"] = start_positions
    tokenized_examples["end_positions"] = end_positions
    return tokenized_examples
```

- 将 `start_positions` 和 `end_positions` **添加到 tokenized 数据**。

## 处理整个数据集

```python
tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets["train"].column_names)
print(tokenized_datasets)
```

- `datasets.map(process_function, batched=True)`: **批量** 应用 `process_function`。
- `remove_columns=datasets["train"].column_names`: **移除原始数据列**。

------

## 加载问答模型

```python
model = AutoModelForQuestionAnswering.from_pretrained("hfl/chinese-macbert-base")
```

- 加载 **MacBERT** 预训练的 **问答模型**。

## **设定训练参数**

```python
args = TrainingArguments(
    output_dir="model_for_qa",
    per_device_train_batch_size=32,
    gradient_accumulation_steps=32,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_steps=10,
    num_train_epochs=2
)
```

- 训练参数
  - `output_dir="model_for_qa"`：输出模型存放路径。
  - `per_device_train_batch_size=32`：单个 GPU 训练 batch size。
  - `gradient_accumulation_steps=32`：梯度累积步数。
  - `evaluation_strategy="epoch"` & `save_strategy="epoch"`：每个 `epoch` 进行评估 & 保存。
  - `num_train_epochs=2`：训练 **2** 个 epoch。

## 训练模型

```python
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=DefaultDataCollator()
)

trainer.train()
```

- 使用 **Trainer API** 训练模型。