# 检索对话机器人

### **1. 模块导入**

```python
import faiss
import torch
import pandas as pd
from tqdm import tqdm
from DualModel import DualModel
from transformers import AutoTokenizer, BertForSequenceClassification
```

- **`faiss`**: Facebook AI Similarity Search 库，用于高效地进行向量相似度搜索。
- **`torch`**: PyTorch 深度学习框架，用于模型训练和推理。
- **`pandas`**: 数据处理库，用于加载和操作 CSV 文件。
- **`tqdm`**: 进度条工具，用于显示循环的进度。
- **`DualModel`**: 自定义的双塔模型（Dual Model），用于将文本编码为向量。
- **`AutoTokenizer` 和 `BertForSequenceClassification`**: Hugging Face Transformers 库中的工具，用于加载预训练模型和分词器。

------

### **2. 加载数据集**

```python
data = pd.read_csv('law_faq.csv') # title作为问题，reply作为回答
```

- 从 `law_faq.csv` 文件中加载数据集，假设文件包含两列：`title`（问题）和 `reply`（回答）。
- 数据将存储在 `pandas.DataFrame` 对象 `data` 中。

------

### **3. 加载模型和分词器**

```python
model = DualModel.from_pretrained("../05_文本相似度/向量匹配策略解决大候选样本的解决方案/checkpoint-250")
model = model.cuda()
model.eval()
tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-macbert-base")
```

- **`model`**: 加载预训练的双塔模型（`DualModel`），用于将文本编码为向量。
- **`model.cuda()`**: 将模型移动到 GPU 上以加速计算。
- **`model.eval()`**: 将模型设置为评估模式，禁用 Dropout 和 Batch Normalization 等训练相关操作。
- **`tokenizer`**: 加载中文预训练分词器 `hfl/chinese-macbert-base`，用于将文本转换为模型输入。

------

### **4. 将问题编码成向量**

```python
questions = data['title'].to_list()
questions_vectors = []
with torch.inference_mode():
    for i in tqdm(range(0, len(questions), 32)): # 这里使用的是批量batch的处理方式，可以提高效率
        batch_sens = questions[i:i+32]
        inputs = tokenizer(batch_sens, padding=True, return_tensors="pt", max_length=128, truncation=True)
        inputs = {k:v.to(model.device)for k,v in inputs.items()}
        question_vector = model.bert(**inputs)[1]
        questions_vectors.append(question_vector)
questions_vectors = torch.concat(questions_vectors, dim=0).cpu().numpy()
print(questions_vectors.shape)
```

- **`questions`**: 提取数据集中的 `title` 列（问题）并转换为列表。
- **`questions_vectors`**: 存储所有问题的向量表示。
- **`torch.inference_mode()`**: 禁用梯度计算，提高推理效率。
- **`tqdm`**: 显示进度条，方便观察处理进度。
- **`batch_sens`**: 每次处理 32 个问题（批量处理以提高效率）。
- **`tokenizer`**: 将批量问题转换为模型输入格式（`input_ids`, `attention_mask` 等）。
- **`inputs.to(model.device)`**: 将输入数据移动到 GPU 上。
- **`model.bert(**inputs)[1]`**: 使用模型的 BERT 部分编码问题，提取 CLS 向量作为问题表示。
- **`torch.concat`**: 将所有批次的向量拼接为一个完整的向量矩阵。
- **`.cpu().numpy()`**: 将向量从 GPU 移动到 CPU 并转换为 NumPy 数组。
- **`print(questions_vectors.shape)`**: 打印向量矩阵的形状。

------

### **5. 创建索引**

```python
index = faiss.IndexFlatIP(768)
faiss.normalize_L2(questions_vectors)
index.add(questions_vectors)
print(index)
```

- **`faiss.IndexFlatIP(768)`**: 创建一个 Faiss 索引，使用内积（IP）作为相似度度量，向量维度为 768。
- **`faiss.normalize_L2`**: 对向量进行 L2 归一化，使得内积等价于余弦相似度。
- **`index.add`**: 将问题向量添加到索引中。
- **`print(index)`**: 打印索引信息。

------

### **6. 编码问题**

```python
question = "在法律中定金与订金的区别订金和定金哪个"
with torch.inference_mode():
    inputs = tokenizer(question, padding=True, return_tensors="pt", max_length=128, truncation=True)
    inputs = {k:v.to(model.device)for k,v in inputs.items()}
    question_vector = model.bert(**inputs)[1].cpu().numpy()
print(question_vector.shape)
```

- **`question`**: 待查询的问题。
- **`tokenizer`**: 将问题转换为模型输入格式。
- **`inputs.to(model.device)`**: 将输入数据移动到 GPU 上。
- **`model.bert(**inputs)[1]`**: 使用模型编码问题，提取 CLS 向量。
- **`.cpu().numpy()`**: 将向量从 GPU 移动到 CPU 并转换为 NumPy 数组。
- **`print(question_vector.shape)`**: 打印向量形状。

------

### **7. 向量匹配**

```python
faiss.normalize_L2(question_vector)
scores, indexes = index.search(question_vector, 10)
topk_results = data.values[indexes[0].tolist()]
print(topk_results[:,0])
```

- **`faiss.normalize_L2`**: 对查询向量进行 L2 归一化。
- **`index.search`**: 在索引中搜索与查询向量最相似的 10 个问题。
- **`topk_results`**: 获取匹配问题的原始数据（问题和回答）。
- **`print(topk_results[:,0])`**: 打印匹配的问题。

------

### **8. 交互模型**

```python
cross_model = BertForSequenceClassification.from_pretrained("../05_文本相似度/向量匹配策略解决大候选样本的解决方案/checkpoint-250")
cross_model = cross_model.cuda()
cross_model.eval()
```

- **`cross_model`**: 加载预训练的 BERT 分类模型，用于进一步筛选匹配结果。
- **`cross_model.cuda()`**: 将模型移动到 GPU 上。
- **`cross_model.eval()`**: 将模型设置为评估模式。

------

### **9. 最终预测**

```python
candidate = topk_results[:,0].tolist()
ques = [question] * len(candidate)
inputs = tokenizer(ques, candidate, padding=True, return_tensors="pt", max_length=128, truncation=True)
inputs = {k:v.to(cross_model.device)for k,v in inputs.items()}
with torch.inference_mode():
    logits = cross_model(**inputs).logits.squeeze()
    print(logits)
    result = torch.argmax(logits, dim=-1)
print(result)
```

- **`candidate`**: 获取匹配的问题列表。
- **`ques`**: 将查询问题复制为与候选问题数量相同的列表。
- **`tokenizer`**: 将查询问题和候选问题对转换为模型输入格式。
- **`inputs.to(cross_model.device)`**: 将输入数据移动到 GPU 上。
- **`cross_model(**inputs)`**: 使用分类模型计算匹配得分。
- **`torch.argmax`**: 选择得分最高的问题。
- **`print(result)`**: 打印最终匹配结果。

------

### **10. 输出最终答案**

```python
candidate_answers = topk_results[:,1].tolist()
match_question = candidate[result.item()]
final_answer = candidate_answers[result.item()]
print(f"问题：{match_question}\n回答：{final_answer}")
```

- **`candidate_answers`**: 获取匹配问题的回答列表。
- **`match_question`**: 获取最终匹配的问题。
- **`final_answer`**: 获取最终匹配的回答。
- **`print`**: 输出问题和回答。