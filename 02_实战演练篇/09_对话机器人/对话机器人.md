# 对话机器人

### 1. **导入库**

```python
from datasets import Dataset, load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, \
    Seq2SeqTrainingArguments, set_seed, DataCollatorForSeq2Seq, DataCollatorWithPadding, \
    Trainer, TrainingArguments, AutoModelForCausalLM, pipeline
```

这里导入了多个库，包括 `datasets` 和 `transformers`。`datasets` 用于加载和处理数据集，`transformers` 用于加载预训练模型、分词器、训练器等。

### 2. **加载数据集**

```python
dataset = Dataset.load_from_disk('./alpaca_data_zh')
dataset = dataset.select(range(min(50, len(dataset))))
```

这段代码从磁盘加载一个名为 `alpaca_data_zh` 的数据集，并选择前 50 条数据（如果数据集有超过 50 条数据）。

### 3. **加载分词器**

```python
tokenizer = AutoTokenizer.from_pretrained("Langboat/bloom-389m-zh")
```

这里加载了一个预训练的分词器 `Langboat/bloom-389m-zh`，用于将文本转换为模型可以理解的 token ID。

### 4. **数据预处理**

```python
def preprocess_function(examples):
    max_length = 256
    inputs_ids, attention_mask, labels = [], [], []
    instruction = tokenizer("\n".join(["Human: " + examples["instruction"], examples["input"]]).strip() + "\n\nAssistant: ")
    response = tokenizer(examples["output"] + tokenizer.eos_token)
    print(tokenizer.decode(instruction["input_ids"] + response["input_ids"]))
    inputs_ids = instruction["input_ids"] + response["input_ids"]
    attention_mask = instruction["attention_mask"] + response["attention_mask"]
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] # 模型只需要学习response
    if len(inputs_ids) > max_length:
        inputs_ids = inputs_ids[:max_length]
        attention_mask = attention_mask[:max_length]
        labels = labels[:max_length]
    return {"input_ids": inputs_ids, "attention_mask": attention_mask, "labels": labels}
```

这个函数对数据集中的每条数据进行预处理：

- 将 `instruction` 和 `input` 拼接成 "Human: [instruction]\n[input]\n\nAssistant: " 的格式，并进行分词。
- 将 `output` 加上结束符 `eos_token` 并进行分词。
- 将 `instruction` 和 `response` 的 token ID 和 attention mask 拼接起来。
- `labels` 用于训练模型，`-100` 表示这些 token 不需要计算损失（即模型不需要学习这些部分）。
- 如果 token ID 的长度超过 `max_length`，则进行截断。

### 5. **应用预处理函数**

```python
tokenized_datasets = dataset.map(preprocess_function, remove_columns=dataset.column_names)
```

将预处理函数应用到整个数据集，并移除原始列名。

### 6. **加载模型**

```python
model = AutoModelForCausalLM.from_pretrained("Langboat/bloom-389m-zh")
```

加载一个预训练的因果语言模型 `Langboat/bloom-389m-zh`，用于生成文本。

### 7. **设置训练参数**

```python
args = TrainingArguments(
    output_dir="./对话机器人",
    num_train_epochs=1,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=8,
    logging_steps=10,
)
```

设置训练参数，包括输出目录、训练轮数、每个设备的批量大小、梯度累积步数和日志记录步数。

### 8. **初始化训练器**

```python
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)
```

初始化训练器，传入模型、训练参数、训练数据集和数据整理器。

### 9. **开始训练**

```python
trainer.train()
```

开始训练模型。

### 10. **预测**

```python
pipeline = pipeline("text-generation",model=model, tokenizer=tokenizer, device=0)
ipt = "Human: {}\n{}".format("怎么学习自然语言处理", "").strip() + "\n\nAssistant: "
print(pipeline(ipt, max_length=64))
```

使用训练好的模型进行预测：

- 创建一个文本生成 pipeline。
- 构造输入文本 "Human: 怎么学习自然语言处理\n\nAssistant: "。
- 使用 pipeline 生成最多 64 个 token 的文本，并打印结果。

### 总结

这段代码通过加载一个预训练的语言模型，对数据集进行预处理，训练模型，并使用训练好的模型进行文本生成。最终目标是生成一个能够回答用户问题的对话机器人。