# 掩码语言训练

### 1. **加载数据集**

```python
datasets = Dataset.load_from_disk('./wiki_cn_filtered')
print(datasets[0])
datasets = datasets.select(range(300)) # 快速测试时使用
```

- **功能**：从磁盘加载预处理的 `Dataset` 对象，并打印第一条数据。为了快速测试，只选择前 300 条数据。

------

### 2. **数据预处理**

```python
tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-macbert-base")

def preprocess_function(examples):
    return tokenizer(examples["completion"], max_length=512, truncation=True)

tokenized_datasets = datasets.map(preprocess_function, batched=True, remove_columns=datasets.column_names)
dataloader = DataLoader(tokenized_datasets, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15))
```

- 功能：
  - 使用 `hfl/chinese-macbert-base` 的分词器对数据进行分词。
  - 使用 `DataCollatorForLanguageModeling` 对数据进行动态掩码（MLM）。

------

### 3. **加载模型**

```python
model = AutoModelForMaskedLM.from_pretrained("hfl/chinese-macbert-base")
training_args = TrainingArguments(
    output_dir='./预训练模型',
    num_train_epochs=1,
    per_device_train_batch_size=16,
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15),
    train_dataset=tokenized_datasets,
)
```

- **功能**：加载预训练模型 `hfl/chinese-macbert-base`，并配置训练参数和 `Trainer`。

------

### 4. **训练模型**

```python
trainer.train()
```

- **功能**：启动模型训练。

------

### 5. **模型推理**

```python
pipeline = pipeline('fill-mask', model='./预训练模型/checkpoint-19', tokenizer='hfl/chinese-macbert-base', device=0)
sentence = "西安[MASK]通大[MASK]的博[MASK]馆馆长是锺[MASK]善"
results = pipeline(sentence)
filled_sentence = sentence
for i, result in enumerate(results):
    best_token = result[0]['token_str']  # 获取得分最高的词
    mask_index = filled_sentence.find("[MASK]")  # 找到第一个[MASK]
    filled_sentence = filled_sentence[:mask_index] + best_token + filled_sentence[mask_index + 6:]  # 填充[MASK]
print(filled_sentence)
```

- **功能**：使用训练好的模型进行推理，填充句子中的 `[MASK]`。

