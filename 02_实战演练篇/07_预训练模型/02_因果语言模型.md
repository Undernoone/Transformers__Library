# 因果语言模型

```python
datasets = Dataset.load_from_disk('./wiki_cn_filtered')
print(datasets)  # 这个数据集有两个column，一个是source，一个是completion
datasets = datasets.select(range(300))  # 快速测试时使用
```

加载数据集：这里使用了`datasets.select(range(300))`：选择数据集中的前300个样本进行快速测试。这在模型开发的早期阶段非常有用，用来验证数据加载和预处理流程。

```python
tokenizer = AutoTokenizer.from_pretrained("Langboat/bloom-389m-zh")
def preprocess_function(examples):
    # 在数据集中的completion后面加上eos_token，eos_token表示句子结束
    contents = [completion + tokenizer.eos_token for completion in examples["completion"]]
    return tokenizer(contents, max_length=512, truncation=True)

tokenized_datasets = datasets.map(preprocess_function, batched=True, remove_columns=datasets.column_names)  # source、completion ----> input_ids、attention_mask
dataloader = DataLoader(tokenized_datasets, batch_size=2,
                        collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False))  # mlm=True表示使用Masked Language Modeling
```

数据集预处理：

- `AutoTokenizer.from_pretrained("Langboat/bloom-389m-zh")`：加载一个预训练的中文分词器`Langboat/bloom-389m-zh`，这是一个针对中文的`Bloom`模型的分词器。
- `preprocess_function`：定义了数据预处理函数，该函数将`completion`字段中的文本添加`eos_token`（表示句子结束的特殊标记）。这是因果语言模型（Causal Language Modeling）所必需的，因为模型需要知道何时生成一个完整的句子。
- `tokenizer(contents, max_length=512, truncation=True)`：将处理后的文本序列化为`input_ids`和`attention_mask`，每个文本最大长度为512个token。如果文本长度超过512，则进行截断。
- `datasets.map(preprocess_function, batched=True, remove_columns=datasets.column_names)`：对整个数据集进行批量处理，调用`preprocess_function`函数并删除原始列（`source`和`completion`），返回的将是`input_ids`和`attention_mask`。
- `DataLoader(tokenized_datasets, batch_size=2, collate_fn=DataCollatorForLanguageModeling(...))`：使用`DataLoader`创建训练数据加载器，批量大小为2，`collate_fn`用于将单个样本打包成一个批次。这里指定`mlm=False`表示训练时不使用掩码语言建模（Masked Language Modeling）。

```python
model = AutoModelForCausalLM.from_pretrained("Langboat/bloom-389m-zh")
training_args = TrainingArguments(
    output_dir='./因果语言模型',
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    logging_steps=10,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    train_dataset=tokenized_datasets,
)
```

定义训练模型：

- `AutoModelForCausalLM.from_pretrained("Langboat/bloom-389m-zh")`：加载一个预训练的因果语言模型（Causal Language Model）。这个模型基于`Langboat/bloom-389m-zh`，是`Bloom`模型的一种变体，用于生成任务。
  - `output_dir='./因果语言模型'`：指定模型输出的目录。
  - `num_train_epochs=1`：训练1个周期（Epoch）。
  - `per_device_train_batch_size=4`：每个设备的训练批量大小为4。
  - `gradient_accumulation_steps=8`：使用梯度累积，每8个步骤进行一次梯度更新，有助于在显存有限的情况下进行更大的批次训练。
  - `logging_steps=10`：每10步记录一次日志。
  - `fp16=True`：启用半精度浮动点数训练，使用`float16`，加速训练并减少显存占用。
- `trainer = Trainer(...)`：创建一个`Trainer`对象，用于训练模型。它接受模型、训练参数、分词器、数据集等。

```python
trainer.train()
```

训练模型：`trainer.train()`：开始训练模型。此方法会根据前面定义的`TrainingArguments`来进行模型训练，包括数据加载、梯度更新等过程。

```python
pipe = pipeline("text-generation", model='./因果语言模型/checkpoint-9', tokenizer=tokenizer, device=0)  # 可以model直接写model，也可以写训练好的checkpoint路径
print(pipe("西安交通大学博物馆（Xi'an Jiaotong University Museum）是一座位于西安", max_length=128, do_sample=True))
print(pipe("下面是一则游戏新闻。小编报道，近日，游戏产业发展的非常", max_length=128, do_sample=True))
```

模型推理：

- `pipeline("text-generation", model='./因果语言模型/checkpoint-9', tokenizer=tokenizer, device=0)`：使用`Hugging Face`的`pipeline`工具进行文本生成。`text-generation`任务指生成文本，指定了模型路径（`checkpoint-9`）和分词器。`device=0`表示使用第一块GPU进行推理。
- `pipe(...)`：输入一段文本进行推理生成。在文本的基础上，模型将生成相关的文本内容。`max_length=128`表示生成文本的最大长度为128个token。`do_sample=True`表示启用采样（模型将从多个可能的输出中进行选择，而不是仅仅选择概率最高的一个）。